{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e33c6d5-234f-4c79-8300-9a025805f5d3",
   "metadata": {},
   "source": [
    "# Parsing and munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7532d6ba-b343-447e-a8de-bc237629e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingredient_parser.en import parse\n",
    "import re\n",
    "import ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809a87a4-d0f1-4e49-9a6d-047228b5b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rparse(i):\n",
    "    pattern = r\"\"\"(?x)     # Use free-spacing mode.\n",
    "        \\s?\\(.*\\)\\s?|   # something in parentheses with a space before/after\n",
    "        ,.*$            # from a comma to the end of the string\n",
    "        \"\"\"\n",
    "    return re.sub(pattern, \"\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a14ac0a-b387-426c-8bb2-6d90f3b10ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ingredients = [\n",
    "      \"1 pound skinless, boneless chicken breast halves - cut into 1 inch cubes\",\n",
    "      \"1 pound andouille sausage, sliced\",\n",
    "      \"1 28 ounce can diced tomatoes with juice\",\n",
    "      \"1 large onion, chopped\",\n",
    "      \"1 large green bell pepper, chopped\",\n",
    "      \"1 cup chopped celery\",\n",
    "      \"1 cup chicken broth\",\n",
    "      \"2 teaspoons dried oregano\",\n",
    "      \"2 teaspoons dried parsley\",\n",
    "      \"2 teaspoons Cajun seasoning\",\n",
    "      \"1 teaspoon cayenne pepper\",\n",
    "      \"1/2 teaspoon dried thyme\",\n",
    "      \"1 pound frozen cooked shrimp without tails\",\n",
    "      \"2 cups (about 9 1/2 ounces) whole almonds, toasted\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e660cb-192e-4763-a826-54081f707dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['ar', 'epi', 'fn']\n",
    "\n",
    "for file_name in files:\n",
    "    with open(f'data/recipes_raw_ndjson/recipes_raw_nosource_{file_name}.json') as f:\n",
    "        data = ndjson.load(f)\n",
    "\n",
    "    # gather ingredient lists, parse, and write to a new file.\n",
    "    data_ingredients = [recipe['ingredients'] for recipe in data]\n",
    "\n",
    "    data_ingredients = [[rparse(parse(ingredient)['name']).lower() for ingredient in ingredients] for ingredients in data_ingredients]\n",
    "\n",
    "    with open(f'data/recipes_raw_ndjson/ingredients_{file_name}.txt', 'w') as f:\n",
    "        for ingredients in data_ingredients:\n",
    "            f.write(','.join(ingredients))\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # gather instructions and write to a new file\n",
    "    data_instructions = [recipe['instructions'] for recipe in data]\n",
    "    \n",
    "    with open(f'data/recipes_raw_ndjson/instructions_{file_name}.txt', 'w') as f:\n",
    "        for instruction in data_instructions:\n",
    "            f.write(instruction)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9be73ae-faea-44d7-9c78-f2f168aec6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skinless',\n",
       " 'andouille sausage',\n",
       " 'can diced tomatoes with juice',\n",
       " 'large onion',\n",
       " 'large green bell pepper',\n",
       " 'chopped celery',\n",
       " 'chicken broth',\n",
       " 'dried oregano',\n",
       " 'dried parsley',\n",
       " 'Cajun seasoning',\n",
       " 'cayenne pepper',\n",
       " 'dried thyme',\n",
       " 'frozen cooked shrimp without tails',\n",
       " 'whole almonds']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rparse(parse(i)['name']) for i in test_ingredients]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8f6266a8-12fd-459d-b1ae-98ca30e82c1f",
   "metadata": {},
   "source": [
    "print(f'''Original ingredient:\n",
    "  '1 1/2 cups sugar'\n",
    "After parsing:\n",
    "  '{parse('1 1/2 cups sugar')['name']}'\n",
    "''')\n",
    "\n",
    "print(f'''Original ingredient:\n",
    "  '2 cups (about 9 1/2 ounces) whole almonds, toasted'\n",
    "After parsing:\n",
    "  '{parse('2 cups (about 9 1/2 ounces) whole almonds, toasted')['name']}'\n",
    "''')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92ad9ad7-bd63-4ab5-a843-84585deaedae",
   "metadata": {},
   "source": [
    "ingredient_list = [[rparse(parse(i)['name']) for i in recipe['ingredients']] for recipe in r.data_epi.values()]\n",
    "# print(ingredient_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cda3a2-62c1-4cc5-abc6-bf2eb3b9eec2",
   "metadata": {},
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80a60fa2-958b-48e1-9f3b-72c9162b651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open python and nltk packages needed for processing\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "filepath = 'data/recipes_raw_ndjson/instructions_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbe954b0-6d1d-4bdd-b123-b942365b8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the file, read the text and close it\n",
    "f = open(filepath, 'r')\n",
    "filetext = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3002f719-f334-444e-824c-651324c6edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize by the regular word tokenizer\n",
    "filetokens = nltk.word_tokenize(filetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0963bf4a-0854-446e-be70-71f5ea02445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25069841\n"
     ]
    }
   ],
   "source": [
    "print(len(filetokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "011d1952-a2f6-4d02-a23d-c322e03aad55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Place', 'the', 'chicken', ',', 'butter', ',', 'soup', ',', 'and', 'onion', 'in', 'a', 'slow', 'cooker', ',', 'and', 'fill', 'with', 'enough', 'water', 'to', 'cover', 'Cover', ',', 'and', 'cook', 'for', '5', 'to', '6']\n"
     ]
    }
   ],
   "source": [
    "print(filetokens[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5400c308-f116-4ab8-9036-d9796be8ecda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display first 30 words from file:\n",
      "25069841\n",
      "['place', 'the', 'chicken', ',', 'butter', ',', 'soup', ',', 'and', 'onion', 'in', 'a', 'slow', 'cooker', ',', 'and', 'fill', 'with', 'enough', 'water', 'to', 'cover', 'cover', ',', 'and', 'cook', 'for', '5', 'to', '6']\n"
     ]
    }
   ],
   "source": [
    "# choose to treat upper and lower case the same\n",
    "#    by putting all tokens in lower case\n",
    "filewords = [w.lower() for w in filetokens]\n",
    "\n",
    "# display the first words\n",
    "print (\"Display first 30 words from file:\")\n",
    "print(len(filewords))\n",
    "print (filewords[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3ccf94e-8e56-417e-8cfa-8e81ede9d318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display first 50 Stopwords:\n",
      "[\"'s\", 'a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking']\n"
     ]
    }
   ],
   "source": [
    "# read a stop word file\n",
    "fstop = open('data/smart.english.stop', 'r')\n",
    "stoptext = fstop.read()\n",
    "fstop.close()\n",
    "\n",
    "stopwords = nltk.word_tokenize(stoptext)\n",
    "stopwords.extend(['small', 'medium', 'large', 'medium-high', 'high', 'low'])\n",
    "print (\"Display first 50 Stopwords:\")\n",
    "print (stopwords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a59fc9f-b59f-48a5-96d4-d0199ae52144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(w):\n",
    "    # pattern to match word of non-alphabetical characters\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "89f15fe5-c0a2-4920-a8b2-4fe002794c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup to process bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(filewords)\n",
    "# choose to use both the non-alpha word filter and a stopwords filter\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e59d172d-c33e-4ee6-b9dd-35ec4bd2d405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 20 frequencies\n",
      "(('preheat', 'oven'), 0.0012645871986184515)\n",
      "(('olive', 'oil'), 0.0012578460310139182)\n",
      "(('baking', 'sheet'), 0.0011720058376118142)\n",
      "(('golden', 'brown'), 0.0008317962606942741)\n",
      "(('room', 'temperature'), 0.000818114482656671)\n",
      "(('stirring', 'occasionally'), 0.0007419672107214401)\n",
      "(('lemon', 'juice'), 0.0007202678309766703)\n",
      "(('baking', 'dish'), 0.000595655951707073)\n",
      "(('reduce', 'heat'), 0.0005919463150962944)\n",
      "(('food', 'processor'), 0.0005651810875066977)\n",
      "(('preheated', 'oven'), 0.00048719894154893126)\n",
      "(('plastic', 'wrap'), 0.0004710440724374758)\n",
      "(('degrees', 'f.'), 0.00044543561325339077)\n",
      "(('brown', 'sugar'), 0.0004264486559767172)\n",
      "(('black', 'pepper'), 0.00037977903409917917)\n",
      "(('cool', 'completely'), 0.0003744738548601086)\n",
      "(('teaspoon', 'salt'), 0.0003715619895634759)\n",
      "(('paper', 'towels'), 0.00035744143730309256)\n",
      "(('electric', 'mixer'), 0.0003572419944745561)\n",
      "(('baking', 'powder'), 0.0003493041698988039)\n"
     ]
    }
   ],
   "source": [
    "# score by frequency and display the top 50 bigrams\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print ()\n",
    "print (\"Bigrams from file with top 20 frequencies\")\n",
    "for item in scored[:20]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "452dd68b-f900-428a-a040-d9d605eae5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 20 mutual information scores\n",
      "(('alois', 'lageder'), 22.25752141554633)\n",
      "(('beau', 'monde'), 22.25752141554633)\n",
      "(('bedford', 'thompson'), 22.25752141554633)\n",
      "(('coyote', 'bait'), 22.25752141554633)\n",
      "(('finnan', 'haddie'), 22.25752141554633)\n",
      "(('nitrous', 'oxide'), 22.25752141554633)\n",
      "(('raymond', 'hom'), 22.25752141554633)\n",
      "(('sabor', 'italia'), 22.25752141554633)\n",
      "(('tara', 'donne'), 22.25752141554633)\n",
      "(('bonny', 'doon'), 21.994487009712536)\n",
      "(('fen', 'szu'), 21.994487009712536)\n",
      "(('tic', 'tac'), 21.994487009712536)\n",
      "(('tung', 'ku'), 21.994487009712536)\n",
      "(('kung', 'pao'), 21.994487009712532)\n",
      "(('gale', 'gand'), 21.77209458837609)\n",
      "(('kari', 'patta'), 21.77209458837609)\n",
      "(('tomric', 'plastics'), 21.77209458837609)\n",
      "(('hahn', 'estates'), 21.772094588376085)\n",
      "(('nobu-style', 'saikyo'), 21.772094588376085)\n",
      "(('anita', 'calero'), 21.57944951043369)\n"
     ]
    }
   ],
   "source": [
    "# score by PMI and display the top 50 bigrams\n",
    "# only use frequently occurring words in mutual information\n",
    "finder.apply_freq_filter(5)\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "print (\"\\nBigrams from file with top 20 mutual information scores\")\n",
    "for item in scored[:20]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02546e9-434f-4d20-a602-13516ff26156",
   "metadata": {},
   "source": [
    "From wikipedia:\n",
    "\n",
    "> Ras el hanout or rass el hanout is a spice mix found in varying forms in Tunisia, Algeria, and Morocco. The name in Arabic means \"head of the shop\" and implies a mixture of the best spices the seller has to offer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94d1c2bb-5261-430d-8005-673f5a964bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 55466 matches:\n",
      "a loaf . Place on top of the ketchup Bake in preheated oven for 1 hour or unti\n",
      " large spoonfuls onto ungreased pans Bake for about 10 minutes in the preheate\n",
      " . Top with the buttery bread crumbs Bake , uncovered , about 25 minutes or un\n",
      ". Pour batter into prepared loaf pan Bake in preheated oven for 60 to 65 minut\n",
      " rimmed baking sheet to catch spills Bake in the preheated oven until bubbling\n",
      "ned . Pour batter into prepared pans Bake for 40 to 60 minutes , or until test\n",
      "nch apart on ungreased cookie sheets Bake 6 to 8 minutes in preheated oven . C\n",
      "lended . Pour into the prepared pans Bake for about 50 minutes in the preheate\n",
      "okies should be about 3 inches apart Bake for 15 to 17 minutes in the preheate\n",
      "ish and sprinkle with Cheddar cheese Bake in preheated oven until cheese is me\n",
      " sprinkle over muffins before baking Bake for 20 to 25 minutes in the preheate\n",
      "oonfuls onto ungreased cookie sheets Bake for 10 to 12 minutes in the preheate\n",
      "e the foil does not touch the cheese Bake in preheated oven for 25 minutes . R\n",
      "n for 25 minutes . Remove foil , and bake an additional 25 minutes . Cool for \n",
      "er . Spread batter into prepared pan Bake in preheated oven for 25 to 30 minut\n",
      "the bread . Dust with reserved flour Bake in a preheated 350 degrees F ( 175 d\n",
      " in the top to allow steam to escape Bake in the preheated oven for 30 to 35 m\n",
      "meal . Sprinkle topping over muffins Bake in preheated oven for 18 to 20 minut\n",
      "ter into the prepared jelly roll pan Bake in the preheated oven until the brow\n",
      "r slowly so that it does not run off Bake 15 minutes in the preheated oven . R\n",
      "h . Pour filling into prepared crust Bake in preheated oven for 1 hour . Turn \n",
      ". Turn pieces over , and brush again Bake in the preheated oven for 30 minutes\n",
      " 30 minutes . Turn pieces over , and bake for another 30 minutes , until no lo\n",
      "ch baking dish , and cover with foil Bake in the preheated oven for 1 hour . W\n",
      "he soup mixture . Replace foil , and bake for another 30 minutes . Preheat ove\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(filetokens)\n",
    "text.concordance(\"bake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fa77ca3-770d-4c90-b27d-658d1bb0ecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['place', 'the', 'chicken', ',', 'butter', ',', 'soup', ',', 'and', 'onion', 'in', 'a', 'slow', 'cooker', ',', 'and', 'fill', 'with', 'enough', 'water', 'to', 'cover', 'cover', ',', 'and', 'cook', 'for', '5', 'to', '6', 'hour', 'on', 'high', '.', 'about', '30', 'minut', 'befor', 'serv', ',', 'place', 'the', 'torn', 'biscuit', 'dough', 'in', 'the', 'slow', 'cooker', '.', 'cook', 'until', 'the', 'dough', 'is', 'no', 'longer', 'raw', 'in', 'the', 'center', '.', 'in', 'a', 'slow', 'cooker', ',', 'mix', 'cream', 'of', 'mushroom', 'soup', ',', 'dri', 'onion', 'soup', 'mix', 'and', 'water', '.', 'place', 'pot', 'roast', 'in', 'slow', 'cooker', 'and', 'coat', 'with', 'soup', 'mixtur', 'cook', 'on', 'high', 'set', 'for', '3', 'to', '4', 'hour', ',', 'or', 'on', 'low', 'set', 'for', '8', 'to', '9', 'hour', '.', 'preheat', 'oven', 'to', '350', 'degre', 'f', '(', '175', 'degre', 'c', ')', '.', 'lightli', 'greas', 'a', '5x9', 'inch', 'loaf', 'pan', 'press', 'the', 'brown', 'sugar', 'in', 'the', 'bottom', 'of', 'the', 'prepar', 'loaf', 'pan', 'and', 'spread', 'the', 'ketchup', 'over', 'the', 'sugar', 'in', 'a', 'mix', 'bowl', ',', 'mix', 'thoroughli', 'all', 'remain', 'ingredi', 'and', 'shape', 'into', 'a', 'loaf', '.', 'place', 'on', 'top', 'of', 'the', 'ketchup', 'bake', 'in', 'preheat', 'oven', 'for', '1', 'hour', 'or', 'until', 'juic', 'are', 'clear', '.', 'preheat', 'oven', 'to', '350', 'degre', 'f', '(', '175', 'degre', 'c', ')', 'cream', 'togeth', 'the', 'butter', ',']\n",
      "['plac', 'the', 'chick', ',', 'but', ',', 'soup', ',', 'and', 'on', 'in', 'a', 'slow', 'cook', ',', 'and', 'fil', 'with', 'enough', 'wat', 'to', 'cov', 'cov', ',', 'and', 'cook', 'for', '5', 'to', '6', 'hour', 'on', 'high', '.', 'about', '30', 'minut', 'bef', 'serv', ',', 'plac', 'the', 'torn', 'biscuit', 'dough', 'in', 'the', 'slow', 'cook', '.', 'cook', 'until', 'the', 'dough', 'is', 'no', 'long', 'raw', 'in', 'the', 'cent', '.', 'in', 'a', 'slow', 'cook', ',', 'mix', 'cream', 'of', 'mushroom', 'soup', ',', 'dry', 'on', 'soup', 'mix', 'and', 'wat', '.', 'plac', 'pot', 'roast', 'in', 'slow', 'cook', 'and', 'coat', 'with', 'soup', 'mixt', 'cook', 'on', 'high', 'set', 'for', '3', 'to', '4', 'hour', ',', 'or', 'on', 'low', 'set', 'for', '8', 'to', '9', 'hour', '.', 'preh', 'ov', 'to', '350', 'degr', 'f', '(', '175', 'degr', 'c', ')', '.', 'light', 'greas', 'a', '5x9', 'inch', 'loaf', 'pan', 'press', 'the', 'brown', 'sug', 'in', 'the', 'bottom', 'of', 'the', 'prep', 'loaf', 'pan', 'and', 'spread', 'the', 'ketchup', 'ov', 'the', 'sug', 'in', 'a', 'mix', 'bowl', ',', 'mix', 'thorough', 'al', 'remain', 'ingredy', 'and', 'shap', 'into', 'a', 'loaf', '.', 'plac', 'on', 'top', 'of', 'the', 'ketchup', 'bak', 'in', 'preh', 'ov', 'for', '1', 'hour', 'or', 'until', 'juic', 'ar', 'clear', '.', 'preh', 'ov', 'to', '350', 'degr', 'f', '(', '175', 'degr', 'c', ')', 'cream', 'togeth', 'the', 'but', ',']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "# compare how the two stemmers work on a small portion of the tokens.\n",
    "\n",
    "crimePstem = [porter.stem(t) for t in filetokens]\n",
    "print(crimePstem[:200])\n",
    "\n",
    "crimeLstem = [lancaster.stem(t) for t in filetokens]\n",
    "print(crimeLstem[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2570d88e-ebaa-480d-a909-dcf50d0c86ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Place', 'the', 'chicken', ',', 'butter', ',', 'soup', ',', 'and', 'onion', 'in', 'a', 'slow', 'cooker', ',', 'and', 'fill', 'with', 'enough', 'water', 'to', 'cover', 'Cover', ',', 'and', 'cook', 'for', '5', 'to', '6', 'hour', 'on', 'High', '.', 'About', '30', 'minute', 'before', 'serving', ',', 'place', 'the', 'torn', 'biscuit', 'dough', 'in', 'the', 'slow', 'cooker', '.', 'Cook', 'until', 'the', 'dough', 'is', 'no', 'longer', 'raw', 'in', 'the', 'center', '.', 'In', 'a', 'slow', 'cooker', ',', 'mix', 'cream', 'of', 'mushroom', 'soup', ',', 'dry', 'onion', 'soup', 'mix', 'and', 'water', '.', 'Place', 'pot', 'roast', 'in', 'slow', 'cooker', 'and', 'coat', 'with', 'soup', 'mixture', 'Cook', 'on', 'High', 'setting', 'for', '3', 'to', '4', 'hour', ',', 'or', 'on', 'Low', 'setting', 'for', '8', 'to', '9', 'hour', '.', 'Preheat', 'oven', 'to', '350', 'degree', 'F', '(', '175', 'degree', 'C', ')', '.', 'Lightly', 'grease', 'a', '5x9', 'inch', 'loaf', 'pan', 'Press', 'the', 'brown', 'sugar', 'in', 'the', 'bottom', 'of', 'the', 'prepared', 'loaf', 'pan', 'and', 'spread', 'the', 'ketchup', 'over', 'the', 'sugar', 'In', 'a', 'mixing', 'bowl', ',', 'mix', 'thoroughly', 'all', 'remaining', 'ingredient', 'and', 'shape', 'into', 'a', 'loaf', '.', 'Place', 'on', 'top', 'of', 'the', 'ketchup', 'Bake', 'in', 'preheated', 'oven', 'for', '1', 'hour', 'or', 'until', 'juice', 'are', 'clear', '.', 'Preheat', 'oven', 'to', '350', 'degree', 'F', '(', '175', 'degree', 'C', ')', 'Cream', 'together', 'the', 'butter', ',']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "crimeLemma = [wnl.lemmatize(t) for t in filetokens]\n",
    "print(crimeLemma[:200])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
